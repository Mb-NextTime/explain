# Мысли
- мб взять конкретный датасет (достаточно разносторонний, мб несколько + хочется чтобы он был на понятную тематику) в качестве "эталонного" и на нем в последствии оценивать методы

---

# Статьи

## “Why Should I Trust You?” Explaining the Predictions of Any Classifier
- https://arxiv.org/pdf/1602.04938.pdf
- AKA оригинальная статья LIME
- что хотим получить от explainer-а?
  - объяснение должно быть **интерпретируемым** - т.е. давать качественное понимание связи входных данных и результата. должны учитывать таргетируемую аудиторию для объяснения и делать поправку на их навыки и знания
  - нужна **локальная достоверность** - т.е. конкретное объяснение должно соответствовать тому, как модель ведет себя в окрестности предсказываемого наблюдения (не путать с глобальной достоверностью, из локальной не следует глобальная!)
  - хотим уметь объяснять любую модель, для этого воспринимаем любую модель как black box. полезно, т.к. учитывать особенности модели = ограничивать область применения для классификаторов в будущем (+ все равно большинство sota моделей это black box-ы)
  - хотим уметь показать глобальную перспективу (например выбрав репрезентативные объяснения)
- считаем объяснением какую-то потенциально объяснимую модель (лин. рег., деревья и тд.).
тогда нужное нам объяснение:
$argmin_{g \in models}(L(f, g, \pi_x) + \Omega(g)$,
где $L$ - достоверность, $\pi_x$ - окрестность $x$, а $\Omega$ - интерпретируемость.
таким образом пытаемся найти **баланс между интерпретируемостью и достоверностью**
- метод для локального объяснения: семплируем данные вокруг наблюдения, раздаем веса в зависимости от удаленности от $x$, таргет - предсказание модели
- когда пытаемся показать глобальную перспективу, встает вопрос: какие наблюдения показать? выбираем так, чтобы покрыть почти все фичи + избежать наблюдений с одинаковым объяснением
- что-то похожее делали во время вступительного задания с Explaining machine learning based diagnosis of COVID 19 from routine blood with decision trees and criteria graphs (генерили синтетику вокруг и обучали дерево, потом оттуда забирали признаки) - видимо было вдохновлено LIME

---

## EXSUM: From Local Explanations to Model Understanding
- https://www.semanticscholar.org/paper/ExSum%3A-From-Local-Explanations-to-Model-Zhou-Ribeiro/df523b4f55989854909fd3343f5e07258bd5bdba
- от автора LIME
- теперь хотим научиться получать обобщенное объяснение модели
- фича -> математическое описание -> высокоуровневое объяснение
- $\Braket{x_i \rarr MD_i \rarr HL_i} \rarr HL^{(g)}$ - плохо т.к. неформальный переход сначала от математического описания к локальному объяснению, а потом от локального к обобщенному
- вместо этого: $\Braket{x_i \rarr MD_i} \rarr MD^{(g)} \rarr HL^{(g)}$
- используем "правила" правило представимо в виде $\Braket{a, b}$, где
$a$ - бинарная функция, проверяющая, применимо ли данное правило к конкретному FEU (Fundamental Explanation Unit, представим как $(x, y, l), 1 \leq l \leq L_x$),
$b$ - предсказывает набор возможных значений объяснения для FEU, называемый диапазоном поведения
- правила можно объединять, пересекать
- предлагается 3 метрики для оценки качества правил:
  - покрытие (доля FEU, которые мы пытаемся объяснить)
  - достоверность (насколько описанное нами поведение совпадает с действительностью)
  - точность (можно трактовать как консистентность поведения описываемой модели)
- главная проблема - необходимо вовлечение человека в написание и оптимизацию правил