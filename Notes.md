# Мысли
- мб взять конкретный датасет (достаточно разносторонний, мб несколько) в качестве "эталонного" и на нем в последствии оценивать методы

# Статьи

## “Why Should I Trust You?” Explaining the Predictions of Any Classifier
- https://arxiv.org/pdf/1602.04938.pdf
- AKA оригинальная статья LIME
- что хотим получить от explainer-а?
  - объяснение должно быть **интерпретируемым** - т.е. давать качественное понимание связи входных данных и результата. должны учитывать таргетируемую аудиторию для объяснения и делать поправку на их навыки и знания
  - нужна **локальная достоверность** - т.е. конкретное объяснение должно соответствовать тому, как модель ведет себя в окрестности предсказываемого наблюдения (не путать с глобальной достоверностью, из локальной не следует глобальная!)
  - хотим уметь объяснять любую модель, для этого воспринимаем любую модель как black box. полезно, т.к. учитывать особенности модели = ограничивать область применения для классификаторов в будущем (+ все равно большинство sota моделей это black box-ы)
  - хотим уметь показать глобальную перспективу (например выбрав репрезентативные объяснения)
- считаем объяснением какую-то потенциально объяснимую модель (лин. рег., деревья и тд.).
тогда нужное нам объяснение:
$argmin_{g \in models}(L(f, g, \pi_x) + \Omega(g)$,
где $L$ - достоверность, $\pi_x$ - окрестность $x$, а $\Omega$ - интерпретируемость.
таким образом пытаемся найти **баланс между интерпретируемостью и достоверностью**
- метод для локального объяснения: семплируем данные вокруг наблюдения, раздаем веса в зависимости от удаленности от $x$, таргет - предсказание модели
- когда пытаемся показать глобальную перспективу, встает вопрос: какие наблюдения показать? выбираем так, чтобы покрыть почти все фичи + избежать наблюдений с одинаковым объяснением

---

